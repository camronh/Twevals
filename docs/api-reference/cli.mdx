---
title: CLI Reference
description: Command-line interface for running evaluations
---

## Basic Usage

```bash
twevals PATH [OPTIONS]
```

Where `PATH` can be:
- A directory: `twevals evals/`
- A file: `twevals evals/customer_service.py`
- A specific function: `twevals evals.py::test_refund`
- A parametrized variant: `twevals evals.py::test_math[2-3-5]`

## Options

### Filtering

<ParamField path="-d, --dataset" type="string" default="all">
  Filter evaluations by dataset. Can be specified multiple times.
</ParamField>

```bash
twevals evals/ --dataset customer_service
twevals evals/ -d customer_service -d technical_support
```

<ParamField path="-l, --label" type="string" default="all">
  Filter evaluations by label. Can be specified multiple times.
</ParamField>

```bash
twevals evals/ --label production
twevals evals/ -l production -l critical
```

<ParamField path="--limit" type="integer">
  Limit the number of evaluations to run.
</ParamField>

```bash
twevals evals/ --limit 10
```

### Execution

<ParamField path="-c, --concurrency" type="integer" default="0">
  Number of concurrent evaluations. `0` means sequential execution.
</ParamField>

```bash
# Run 4 evaluations in parallel
twevals evals/ --concurrency 4
twevals evals/ -c 4
```

<ParamField path="--timeout" type="float">
  Global timeout in seconds for all evaluations.
</ParamField>

```bash
twevals evals/ --timeout 30.0
```

### Output

<ParamField path="-v, --verbose" type="flag">
  Enable verbose output. Shows stdout from evaluations.
</ParamField>

```bash
twevals evals/ --verbose
twevals evals/ -v
```

<ParamField path="-q, --quiet" type="flag">
  Reduce output. Only shows summary.
</ParamField>

```bash
twevals evals/ --quiet
twevals evals/ -q
```

<ParamField path="--list" type="flag">
  List evaluations without running them.
</ParamField>

```bash
twevals evals/ --list
```

### Export

<ParamField path="-o, --output" type="string">
  Save JSON summary to file.
</ParamField>

```bash
twevals evals/ --output results.json
twevals evals/ -o results.json
```

<ParamField path="--csv" type="string">
  Save results to CSV file.
</ParamField>

```bash
twevals evals/ --csv results.csv
```

<ParamField path="--json" type="flag">
  Output compact JSON to stdout.
</ParamField>

```bash
twevals evals/ --json
twevals evals/ --json | jq '.results[0]'
```

### Web UI

<ParamField path="--serve" type="flag">
  Launch web UI after running evaluations.
</ParamField>

```bash
twevals evals/ --serve
```

<ParamField path="--port" type="integer" default="8000">
  Port for web UI server.
</ParamField>

```bash
twevals evals/ --serve --port 3000
```

## Examples

### Run All Evaluations

```bash
twevals evals/
```

### Run Specific File

```bash
twevals evals/customer_service.py
```

### Run Specific Function

```bash
twevals evals/customer_service.py::test_refund
```

### Run Parametrized Variant

```bash
twevals evals/math.py::test_addition[2-3-5]
```

### Filter by Dataset and Label

```bash
twevals evals/ --dataset qa --label production
```

### Run with Concurrency and Timeout

```bash
twevals evals/ -c 8 --timeout 60.0
```

### Export Results

```bash
twevals evals/ -o results.json --csv results.csv
```

### Launch UI on Custom Port

```bash
twevals evals/ --serve --port 8080
```

### Verbose Debug Run

```bash
twevals evals/ -v --limit 5
```

### Production Pipeline

```bash
twevals evals/ -c 16 --timeout 120 -o results.json --json
```

## Exit Codes

| Code | Meaning |
|------|---------|
| 0 | All evaluations passed |
| 1 | One or more evaluations failed |
| 2 | Error during execution |

## Environment Variables

| Variable | Description |
|----------|-------------|
| `TWEVALS_CONCURRENCY` | Default concurrency level |
| `TWEVALS_TIMEOUT` | Default timeout in seconds |

## Output Format

### Table Output (Default)

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                     customer_service                           ┃
┣━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┫
┃ Name                ┃ Status   ┃ Score    ┃ Latency           ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ test_refund         │ ✓ passed │ 1.0      │ 0.23s             │
│ test_complaint      │ ✗ failed │ 0.0      │ 0.45s             │
└─────────────────────┴──────────┴──────────┴───────────────────┘

Summary: 1/2 passed (50.0%)
```

### JSON Output

```json
{
  "run_id": "2024-01-15T10-30-00Z",
  "total": 2,
  "passed": 1,
  "failed": 1,
  "results": [...]
}
```

### List Output

```
Evaluations to run:
  - customer_service::test_refund
  - customer_service::test_complaint
  - qa::test_greeting[hello]
  - qa::test_greeting[hi]

Total: 4 evaluations
```
