---
title: Web UI
description: View, filter, edit, and export results in your browser
---

Twevals includes a built-in web interface for exploring evaluation results.

## Launching the UI

Add the `--serve` flag to any run:

```bash
twevals evals.py --serve
```

The browser opens automatically at `http://127.0.0.1:8000`. Evaluations run in the background while the UI loads immediately—results appear and update in real-time as each evaluation completes.

### With Sessions

Group related runs together using session names:

```bash
# Named session and run
twevals evals.py --serve --session model-upgrade --run-name gpt5-baseline

# Continue the session with a new run
twevals evals.py --serve --session model-upgrade --run-name gpt5-tuned
```

When not specified, friendly names are auto-generated (e.g., "swift-falcon", "bright-flame").

## Features

### Results Table

The main view shows all evaluation results in a sortable, filterable table:

| Column | Description |
|--------|-------------|
| Function | Function name with dataset shown below |
| Input | The test input |
| Reference | Expected output (if set) |
| Output | The system output |
| Scores | Score chips showing pass/fail status |
| Latency | Execution time |

### Expanding Rows

Click any row to expand and see full details:

- **Input**: The test input
- **Output**: The system output
- **Reference**: Expected output (if set)
- **Scores**: All scores with keys, values, and notes
- **Metadata**: Custom metadata fields
- **Run Data**: Debug/trace information
- **Annotations**: Manual notes added via UI

### Inline Editing

Edit fields directly in the expanded view:

- **Dataset**: Reassign to different dataset
- **Labels**: Add or remove labels
- **Scores**: Adjust scores or add new ones
- **Metadata**: Add custom fields
- **Annotations**: Add notes for review

Changes are saved to the results file.

### Filtering

Filter results by:

- **Status**: Pass, Fail, Error
- **Dataset**: Select specific datasets
- **Labels**: Filter by tags
- **Search**: Text search across all fields

### Sorting

Click column headers to sort:

- Name (alphabetical)
- Status (pass/fail)
- Score (numeric)
- Latency (fastest/slowest)

### Actions

<CardGroup cols={2}>
  <Card title="Refresh" icon="rotate">
    Reload results from disk
  </Card>
  <Card title="Run/Stop" icon="play">
    Start selected evals or stop running ones
  </Card>
  <Card title="Export" icon="download">
    Download as JSON or CSV
  </Card>
  <Card title="Theme" icon="moon">
    Toggle dark/light mode
  </Card>
</CardGroup>

### Run Controls

Control evaluation execution directly from the UI:

**Selection**
- Use checkboxes to select individual results
- Click the header checkbox to select/deselect all visible rows
- Selection persists through filtering

**Start/Stop**
- **Run Selected**: With results selected, click the play button to rerun only those evaluations
- **Run All**: With nothing selected, click play to rerun the entire suite
- **Stop**: While evals are running, click stop to cancel pending and running evaluations

Cancelled evaluations are marked with a "cancelled" status and can be rerun later.

## Custom Port

Specify a different port:

```bash
twevals evals.py --serve --port 3000
```

## Results Storage

Results are saved to `.twevals/runs/` with run names and timestamps:

```
.twevals/
└── runs/
    ├── gpt5-baseline_2024-01-15T10-30-00Z.json  # Named run
    ├── swift-falcon_2024-01-15T14-45-00Z.json   # Auto-generated name
    └── latest.json  # Copy of most recent run
```

Each run file includes session metadata:

```json
{
  "session_name": "model-upgrade",
  "run_name": "gpt5-baseline",
  "run_id": "2024-01-15T10-30-00Z",
  "total_evaluations": 50,
  "results": [...]
}
```

The UI loads from `latest.json` by default.

## Workflow Tips

<AccordionGroup>
  <Accordion title="Iterative Development">
    Keep the UI open while developing. After code changes:
    1. Run `twevals evals.py` in terminal
    2. Click "Refresh" in UI to see new results
  </Accordion>

  <Accordion title="Review Failed Tests">
    Filter by "Fail" status to focus on failures. Expand rows to see inputs/outputs and understand why.
  </Accordion>

  <Accordion title="Annotate for Team">
    Add annotations to flag results for team review. Export annotated results for sharing.
  </Accordion>

  <Accordion title="Compare Runs">
    Open multiple results files by navigating to different run timestamps in `.twevals/runs/`.
  </Accordion>
</AccordionGroup>

## Export Formats

### JSON Export

Full structured data:

```json
{
  "run_id": "2024-01-15T10-30-00Z",
  "results": [
    {
      "name": "test_greeting",
      "input": "Hello",
      "output": "Hi there!",
      "scores": [{"key": "quality", "passed": true}],
      ...
    }
  ]
}
```

### CSV Export

Flattened for spreadsheets:

```csv
name,input,output,status,score,latency,dataset
test_greeting,Hello,Hi there!,passed,1.0,0.23,greetings
```

## Keyboard Shortcuts

| Key | Action |
|-----|--------|
| `r` | Refresh results |
| `e` | Export menu |
| `f` | Focus filter |
| `Esc` | Close expanded row |
