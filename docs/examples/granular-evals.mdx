---
title: Granular Evals
description: When each test case needs its own logic
---

# Granular Evals

Sometimes you can't parametrize. Each test case has unique logic—different target functions, different ways of deriving ground truth, different validation. Twevals handles this naturally.

## The Problem

You're evaluating an agent that handles diverse tasks. Each test case:
- Calls a different endpoint or tool
- Derives ground truth from a different source
- Has custom validation logic

With most frameworks, you'd need a separate dataset for each. With Twevals, you just write functions.

## The Eval

```python
from twevals import eval, EvalContext

@eval(dataset="agent_tasks")
async def test_weather_lookup(ctx: EvalContext):
    ctx.input = "What's the weather in Tokyo?"

    # Run the agent
    ctx.add_output(await agent.run(ctx.input))

    # Get ground truth from the actual weather API
    actual_weather = await weather_api.get_current("Tokyo")
    ctx.reference = f"{actual_weather.temp}°F, {actual_weather.condition}"

    # Validate the agent extracted the right info
    assert str(actual_weather.temp) in ctx.output, "Should mention temperature"
    assert actual_weather.condition.lower() in ctx.output.lower(), "Should mention conditions"


@eval(dataset="agent_tasks")
async def test_stock_price(ctx: EvalContext):
    ctx.input = "What's Apple's stock price?"

    ctx.add_output(await agent.run(ctx.input))

    # Get ground truth from the stock API
    actual_price = await stock_api.get_price("AAPL")
    ctx.reference = f"${actual_price:.2f}"

    # Allow some tolerance since prices change
    mentioned_price = extract_price_from_text(ctx.output)
    diff = abs(mentioned_price - actual_price)
    assert diff < 5, f"Price ${mentioned_price} too far from actual ${actual_price}"


@eval(dataset="agent_tasks")
async def test_calendar_booking(ctx: EvalContext):
    ctx.input = "Book a meeting with John tomorrow at 2pm"

    ctx.add_output(await agent.run(ctx.input))

    # Check the calendar API to verify the booking was made
    bookings = await calendar_api.get_bookings(date="tomorrow")
    john_booking = next((b for b in bookings if "john" in b.attendees.lower()), None)

    ctx.reference = "Meeting booked with John at 2:00 PM"
    ctx.metadata["booking_details"] = john_booking

    assert john_booking is not None, "Booking should exist in calendar"
    assert john_booking.time == "14:00", f"Wrong time: {john_booking.time}"


@eval(dataset="agent_tasks")
async def test_email_summary(ctx: EvalContext):
    ctx.input = "Summarize my unread emails"

    # Get the actual emails first (we'll compare against this)
    actual_emails = await email_api.get_unread()
    ctx.metadata["email_count"] = len(actual_emails)
    ctx.metadata["subjects"] = [e.subject for e in actual_emails]

    ctx.add_output(await agent.run(ctx.input))

    # Verify the summary mentions key emails
    for email in actual_emails[:3]:  # Check top 3
        assert email.sender in ctx.output or email.subject in ctx.output, \
            f"Summary missing email from {email.sender}: {email.subject}"


@eval(dataset="agent_tasks")
async def test_code_execution(ctx: EvalContext):
    ctx.input = "Calculate the factorial of 10"

    ctx.add_output(await agent.run(ctx.input))

    # Ground truth is just math
    import math
    ctx.reference = str(math.factorial(10))  # 3628800

    assert ctx.reference in ctx.output, f"Should contain {ctx.reference}"
```

## What's Happening

**Same dataset, different logic** — All five evals write to `"agent_tasks"`. When you run them, you get one unified result set with all five test cases.

**Ground truth from external sources** — Each test derives its reference differently: weather API, stock API, calendar check, email fetch, or pure computation. No hardcoded expected values.

**Custom validation per test** — Weather checks for temperature mentions, stock allows tolerance, calendar verifies side effects, email checks coverage. Each test validates what matters for that task.

**Rich metadata** — We store `booking_details`, `email_count`, `subjects`, etc. This context shows up in results for debugging.

## Running It

```bash
# Run all agent_tasks evals
twevals evals/agent_tasks.py --serve

# Run just one specific test
twevals evals/agent_tasks.py::test_weather_lookup

# Run with verbose output
twevals evals/agent_tasks.py --verbose
```

## When to Use This Pattern

Use granular evals when:
- Each test needs different external data sources for ground truth
- Validation logic varies significantly per test
- You're testing diverse agent capabilities in one suite
- You want to add new test cases without touching existing ones

Use parametrized evals when:
- All test cases share the same logic
- You have a large dataset of input/output pairs
- Ground truth is known upfront (not derived at runtime)
