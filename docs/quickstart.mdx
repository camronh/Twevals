---
title: Quickstart
description: Get up and running with Twevals in under 5 minutes
---

## Installation

Install Twevals as a development dependency:

<CodeGroup>

```bash pip
pip install twevals
```

```bash uv
uv add twevals --dev
```

```bash poetry
poetry add twevals --group dev
```

</CodeGroup>

## Your First Evaluation

Create a file called `evals.py`:

```python
from twevals import eval, EvalContext

@eval(dataset="my_first_evals")
async def test_greeting(ctx: EvalContext):
    ctx.input = "Hello, how are you?"
    ctx.add_output(await my_agent(ctx.input))

    # Use assertions to score - just like pytest!
    assert "hello" in ctx.output.lower() or "hi" in ctx.output.lower(), \
        "Response should contain a greeting"
```

That's it! Assertions work like pytest - if they pass, your eval passes. If they fail, the assertion message becomes the failure reason.

## Run Your Evaluations

```bash
twevals evals.py
```

You'll see a formatted table with your results:

<img src="/assets/terminal-output.svg" alt="Terminal output showing eval results" />

## Launch the Web UI

Add the `--serve` flag to open an interactive web interface:

```bash
twevals evals.py --serve
```

This opens a browser at `http://127.0.0.1:8000` where you can:

- View all evaluation results in a filterable table
- Expand rows to see inputs, outputs, and metadata
- Edit scores, labels, and annotations inline
- Export results to JSON or CSV

## Add More Test Cases

Use `@parametrize` to generate multiple evaluations from one function:

```python
from twevals import eval, parametrize, EvalContext

@eval(dataset="greetings")
@parametrize("input,reference", [
    ("Hello!", "friendly"),
    ("I need help urgently", "helpful"),
    ("What's your return policy?", "informative"),
])
async def test_response_tone(ctx: EvalContext):
    # ctx.input and ctx.reference are auto-populated for these special names
    ctx.add_output(await my_agent(ctx.input))
    tone = analyze_tone(ctx.output)

    assert tone == ctx.reference, f"Expected {ctx.reference} tone, got {tone}"
```

<Note>
Only special parameter names (`input`, `reference`, `metadata`, `run_data`, `latency`) auto-populate the context. For other parameters, include them in the function signature.
</Note>

## Pre-populate Context

For cleaner code, set inputs directly in the decorator:

```python
@eval(
    input="What's the weather like?",
    dataset="qa"
)
async def test_weather_query(ctx: EvalContext):
    ctx.add_output(await my_agent(ctx.input))

    assert "weather" in ctx.output.lower(), "Should mention weather"
```

## Filter and Run Specific Tests

```bash
# Run a specific function
twevals evals.py::test_greeting

# Filter by dataset
twevals evals.py --dataset greetings

# Filter by labels
twevals evals.py --label production

# Run with concurrency
twevals evals.py --concurrency 4
```

## Export Results

```bash
# Save JSON summary
twevals evals.py --output results.json

# Export to CSV
twevals evals.py --csv results.csv

# Output compact JSON to stdout
twevals evals.py --json
```

## Next Steps

<CardGroup cols={2}>
  <Card title="EvalContext" icon="code" href="/core-concepts/eval-context">
    Learn about the builder pattern that powers Twevals
  </Card>
  <Card title="Decorators" icon="at" href="/core-concepts/decorators">
    Master the @eval decorator and its options
  </Card>
  <Card title="Scoring" icon="chart-simple" href="/core-concepts/scoring">
    Understand flexible scoring systems
  </Card>
  <Card title="Patterns" icon="diagram-project" href="/guides/patterns">
    See common evaluation patterns
  </Card>
</CardGroup>
