---
title: Parametrize
description: Generate multiple test cases from a single function
---

The `@parametrize` decorator lets you generate multiple evaluations from one function, similar to pytest's parametrize.

## Basic Usage

```python
from twevals import eval, parametrize, EvalContext

@eval(dataset="math")
@parametrize("a,b,expected", [
    (2, 3, 5),
    (10, 20, 30),
    (0, 0, 0),
])
def test_addition(ctx: EvalContext, a, b, expected):
    result = a + b
    ctx.input = f"{a} + {b}"
    ctx.add_output(result)
    ctx.add_score(result == expected, f"Expected {expected}")
```

This generates three evaluations:
- `test_addition[2-3-5]`
- `test_addition[10-20-30]`
- `test_addition[0-0-0]`

## Parameter Formats

### Tuple List

```python
@parametrize("name,age", [
    ("Alice", 30),
    ("Bob", 25),
])
```

### Dictionary List

More readable for complex cases:

```python
@parametrize("operation,a,b,expected", [
    {"operation": "add", "a": 2, "b": 3, "expected": 5},
    {"operation": "multiply", "a": 4, "b": 5, "expected": 20},
    {"operation": "subtract", "a": 10, "b": 3, "expected": 7},
])
```

## Custom IDs

Name your test cases:

```python
@parametrize("threshold", [0.2, 0.5, 0.8], ids=["low", "mid", "high"])
def test_thresholds(ctx: EvalContext, threshold):
    ...
```

Generates:
- `test_thresholds[low]`
- `test_thresholds[mid]`
- `test_thresholds[high]`

## Stacking Decorators (Cartesian Product)

Stack multiple `@parametrize` decorators for all combinations:

```python
@eval(dataset="model_comparison")
@parametrize("model", ["gpt-4", "gpt-3.5", "claude"])
@parametrize("temperature", [0.0, 0.5, 1.0])
def test_models(ctx: EvalContext, model, temperature):
    ...
```

This generates 9 evaluations (3 models × 3 temperatures):
- `test_models[gpt-4-0.0]`
- `test_models[gpt-4-0.5]`
- `test_models[gpt-4-1.0]`
- `test_models[gpt-3.5-0.0]`
- ... and so on

## Auto-Mapping to Context

Parameters with special names automatically populate the context:

```python
@eval(dataset="qa")
@parametrize("input,reference,metadata", [
    ("What is 2+2?", "4", {"difficulty": "easy"}),
    ("What is the capital of France?", "Paris", {"difficulty": "medium"}),
])
def test_qa(ctx: EvalContext):
    # ctx.input, ctx.reference, and ctx.metadata are auto-populated
    ctx.add_output(await my_agent(ctx.input))
    ctx.add_score(ctx.output == ctx.reference)
```

Special parameter names:
- `input` → `ctx.input`
- `reference` → `ctx.reference`
- `metadata` → merged into `ctx.metadata`

## Accessing Parameters

Parameters are available in two ways:

### As Function Arguments

```python
@parametrize("prompt,expected", [...])
def test_example(ctx: EvalContext, prompt, expected):
    # Use prompt and expected directly
```

### As Context Attributes

```python
@parametrize("prompt,expected", [...])
def test_example(ctx: EvalContext):
    # Access via ctx.prompt and ctx.expected
    result = process(ctx.prompt)
    ctx.add_score(result == ctx.expected)
```

## Running Specific Variants

Run a specific parametrized test:

```bash
# Run all variants
twevals evals.py::test_math

# Run specific variant
twevals evals.py::test_math[2-3-5]
```

## Combining with Other Options

```python
@eval(
    dataset="sentiment",
    default_score_key="accuracy",
    metadata_from_params=["model"],
    timeout=5.0
)
@parametrize("model", ["gpt-4", "gpt-3.5"])
@parametrize("text,expected", [
    ("I love this!", "positive"),
    ("This is terrible", "negative"),
])
async def test_sentiment(ctx: EvalContext, model, text, expected):
    ctx.metadata["text_length"] = len(text)

    result = await analyze_sentiment(text, model=model)
    ctx.add_output(result)
    ctx.add_score(result == expected, f"Expected {expected}")
```

## Data-Driven Testing

Load test cases from external sources:

```python
import json

# Load from file
with open("test_cases.json") as f:
    test_cases = json.load(f)

@eval(dataset="qa")
@parametrize("question,answer", test_cases)
async def test_qa(ctx: EvalContext, question, answer):
    ctx.input = question
    ctx.reference = answer
    ctx.add_output(await my_agent(question))
    ctx.add_score(ctx.output == answer)
```

## Example: Comprehensive Test Suite

```python
from twevals import eval, parametrize, EvalContext

PROMPTS = [
    {"input": "Hello", "expected_intent": "greeting", "expected_sentiment": "positive"},
    {"input": "I need help", "expected_intent": "support", "expected_sentiment": "neutral"},
    {"input": "This is broken!", "expected_intent": "complaint", "expected_sentiment": "negative"},
]

@eval(dataset="intent_detection", default_score_key="accuracy")
@parametrize("input,expected_intent,expected_sentiment", PROMPTS)
async def test_intent(ctx: EvalContext):
    result = await detect_intent(ctx.input)
    ctx.add_output(result)
    ctx.add_score(result["intent"] == ctx.expected_intent, key="intent_accuracy")
    ctx.add_score(result["sentiment"] == ctx.expected_sentiment, key="sentiment_accuracy")
```
